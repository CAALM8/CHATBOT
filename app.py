import streamlit as st
from huggingface_hub import InferenceClient

st.set_page_config(page_title="Hugging Face Chatbot", layout="wide")

st.title("ğŸ˜€ Hugging Face Chatbot (Streamlit)")

# --- Sidebar settings ---
st.sidebar.header("è®¾ç½®")

HF_TOKEN = st.sidebar.text_input("ä½ çš„ HuggingFace Tokenï¼ˆå¿…å¡«ï¼‰", type="password")
MODEL_ID = st.sidebar.text_input("æ¨¡å‹ ID", "Qwen/Qwen2.5-7B-Instruct")

# å¦‚æœä½ æƒ³æµ‹è¯•åˆ«çš„æ¨¡å‹ï¼Œå¡«å‡ ä¸ªå¯ç”¨çš„é¢„è®¾ï¼š
# - meta-llama/Llama-3.1-8B-Instruct
# - mistralai/Mistral-Nemo-Instruct-2407
# - google/gemma-2-2b-it
# - Qwen/Qwen2.5-7B-Instruct


# --- Chat UI ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

prompt = st.chat_input("è¯·è¾“å…¥æ¶ˆæ¯...")

if prompt and HF_TOKEN:
    st.session_state["messages"].append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›åº”..."):
            try:
                client = InferenceClient(
                    model=MODEL_ID,
                    token=HF_TOKEN
                )

                # è°ƒç”¨ HF Inference APIï¼ˆè‡ªåŠ¨é€‰æ‹©æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰
                response = client.text_generation(
                    prompt,
                    max_new_tokens=256,
                    temperature=0.7,
                )

                st.session_state["messages"].append({"role": "assistant", "content": response})
                st.write(response)

            except Exception as e:
                st.error(f"âŒ å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
